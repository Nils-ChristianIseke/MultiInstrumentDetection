{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99036c1",
   "metadata": {},
   "source": [
    "# Mono and Stereo Model implementation\n",
    "The following notebook implements a pipeline for training a Faster R-CNN with ResNet50 Backbone on our custom Dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513c9ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installs\n",
    "There are most likely more packages that have to be installed. `torch` and `torchvision` are listed here specifically since these versions worked with the cuda version available on bwVisu. If needed uncomment the line and install the correct version. It should be noted that in newer versions of torch the object detection models were extended and might be interesting to have a look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d30b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the available cuda version, specific versions of torch have to be used\n",
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354bd92",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fe621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic python imports\n",
    "import os\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "import random\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Image and array handling imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as func\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Torch model related imports\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Additional imports\n",
    "from pycocotools.coco import COCO # Handling of coco annotation files\n",
    "import albumentations as alb # Image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0391f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility functions\n",
    "The following is a collection of functions needed in various steps of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8bde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic helper functions\n",
    "def json_loader(path):\n",
    "    '''Returns a json object created from a file'''\n",
    "    file = open(path)\n",
    "    return json.load(file)\n",
    "    \n",
    "def pil_loader(path):\n",
    "    '''Returns a Pillow image in RGB from a given filepath'''\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as file:\n",
    "        with Image.open(file) as image:\n",
    "            return image.convert('RGB')\n",
    "\n",
    "def read_lines_from_text_file(filename):\n",
    "    '''Read all the lines in a text file and return as a list'''\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    '''When using multiple parallel workers they can be seeded to provide consistent results'''\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "# Functions for handling the annotation files\n",
    "def fileName2imageID(filename, json_decode):\n",
    "    '''Returns the corresponding image id for a filename from a given annotation structure'''\n",
    "    result = next(z for z in json_decode['images'] if filename in z['file_name'])\n",
    "    return result['id']\n",
    "\n",
    "def catId2Label(cat_Id, json_decode):\n",
    "    '''Returns the corresponding category label to an id from a given annotation structure'''\n",
    "    result = next(z for z in json_decode['categories'] if z['id'] == cat_Id)\n",
    "    return result['name']\n",
    "\n",
    "def getAnnos(filename, json_decode, coco):\n",
    "    '''Returns the annotations for a given image from a given annotation structure'''\n",
    "    annos = []\n",
    "    img = fileName2imageID(filename,json_decode)\n",
    "    with redirect_stdout(StringIO()) as f:\n",
    "        anno_ids = coco.getAnnIds(img)\n",
    "        annos = coco.loadAnns(anno_ids)\n",
    "    return annos\n",
    "\n",
    "def mask_loader(path, img_name):\n",
    "    '''Convert the annotations read from the annotation file to the format used by the model'''\n",
    "    annos = json_loader(path)\n",
    "    with redirect_stdout(StringIO()) as f: # COCO outputs an annoying amount of information. This captures the output.\n",
    "        coco = COCO(path) \n",
    "    anno = getAnnos(img_name,annos, coco)\n",
    "    targets = []\n",
    "    for ann in anno:\n",
    "        bbox = ann['bbox']\n",
    "        cat_id = ann['category_id']\n",
    "        targets.append({'bboxes': bbox, 'labels': cat_id})\n",
    "    return targets\n",
    "    \n",
    "def mask_np_loader(path):\n",
    "    '''If working with masks instead of bounding boxes this function loads the mask and strips the not needed channels'''\n",
    "    # The np mask has 3 channels but we need only a single channel\n",
    "    return np.load(path)[..., 0]\n",
    "\n",
    "\n",
    "# Function to account for an issue with batch loading\n",
    "def collate(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "# Postprocessing functions\n",
    "def apply_nms(prediction, threshold):\n",
    "    '''Applying Non-Maximum-Suppression to a given set of predictions and filtering out boxes with an iou greater than the threshold'''\n",
    "    # torchvision returns the indices of the boxes to keep not the actual predictions\n",
    "    keep = torchvision.ops.nms(prediction['boxes'], prediction['scores'], threshold)\n",
    "    \n",
    "    final_prediction = prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "def split_classes(predictions):\n",
    "    '''Generator function to split the set of predictions into smaller sets containing predictions for only one class'''\n",
    "    labels_available = set(predictions['labels'].tolist())\n",
    "    labels = predictions['labels']\n",
    "\n",
    "    for class_label in labels_available:\n",
    "        keep = [True if label == class_label else False for label in labels]\n",
    "        class_predictions = predictions.copy()\n",
    "        \n",
    "        class_predictions['boxes'] = class_predictions['boxes'][keep]\n",
    "        class_predictions['labels'] = class_predictions['labels'][keep]\n",
    "        class_predictions['scores'] = class_predictions['scores'][keep]\n",
    "        yield class_predictions\n",
    "        \n",
    "def save_predictions_to_json(filenames, ground_truth, predictions, filepath):\n",
    "    '''Function to write the filenames, ground truth labels and predicted bounding boxes in a file'''\n",
    "    ground_truth = [{val_k: val_v.tolist() for val_k, val_v in val_t.items()} for val_t in ground_truth] \n",
    "    predictions = [{val_k: val_v.tolist() for val_k, val_v in val_t.items()} for val_t in predictions] \n",
    "\n",
    "    pred_dict = {'filenames': filenames, 'labels': ground_truth, 'predictions': predictions}\n",
    "\n",
    "    with open (filepath,'w') as fp:\n",
    "        json.dump(pred_dict, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc58116",
   "metadata": {},
   "source": [
    "## Custom Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93bab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic EndoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f6cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EndoDataset(Dataset):\n",
    "    \"\"\" The endoscopic dataset requires the following folder structure:\n",
    "    Surgery --> Video --> images which contain the images to be loaded\n",
    "    This mono class, works with split files which are text files that contain the\n",
    "    relative path and the image name. (It is a mono class because it returns a single image)\n",
    "    The class reads the image file paths that are specified in the text file and loads the images.\n",
    "    It applies the specified transforms to the image, else it just converts it into a tensor.\n",
    "    :returns Pre-processed and augmented image as a tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_root_folder=None,\n",
    "                 filenames=None,\n",
    "                 height=448,\n",
    "                 width=448,\n",
    "                 image_aug=None,\n",
    "                 aug_prob=0.5,\n",
    "                 camera=\"left\",\n",
    "                 image_ext='.png'):\n",
    "        super(EndoDataset).__init__()\n",
    "        self.data_root_folder = data_root_folder\n",
    "        self.filenames = filenames\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.image_ext = image_ext\n",
    "        self.camera = camera\n",
    "        #self.format = data_format \n",
    "\n",
    "        # Pick image loader based on image format\n",
    "        self.image_loader = np.load if self.image_ext == '.npy' else pil_loader\n",
    "        self.side_map = {\"2\": 2, \"3\": 3, \"l\": 2, \"r\": 3}\n",
    "        self.cam_to_side = {\"left\": \"l\", \"right\": \"r\"}\n",
    "\n",
    "        # Image pre-processing options\n",
    "        self.image_aug = image_aug\n",
    "        self.aug_prob = aug_prob\n",
    "\n",
    "        # Pick resize function based on image format\n",
    "        if self.image_ext == '.png':\n",
    "            # Output: Resized PIL Image\n",
    "            self.resize = transforms.Resize((self.height, self.width), interpolation=Image.LINEAR)\n",
    "            # Resize to dims slightly larger than given dims\n",
    "            # Sometimes useful for aug together with crop function\n",
    "            self.resize_bigger = transforms.Resize((int(self.height * 1.2),\n",
    "                                                    int(self.width * 1.2)))\n",
    "        elif self.image_ext == '.npy':\n",
    "            self.resize = lambda x: cv2.resize(x, (self.width, self.height), interpolation=cv2.INTER_NEAREST)\n",
    "            self.resize_bigger = lambda x: cv2.resize(x, (self.width*1.2, self.height*1.2),\n",
    "                                                      interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    def get_split_filename(self, filename):\n",
    "        \"\"\" Splits a filename string comprising of \"relative_path <space> image_name\"\n",
    "        :param filename A string comprising of \"relative_path <space> image_name\"\n",
    "        :return split_filename- \"relative_path, image_name\"\n",
    "        \"\"\"\n",
    "        split_filename = filename.split()\n",
    "        # folder, frame_num, side\n",
    "        if len(split_filename) == 2: \n",
    "            return split_filename[0], split_filename[1], self.cam_to_side[self.camera]\n",
    "        else:\n",
    "            return split_filename\n",
    "    \n",
    "\n",
    "    def make_image_path(self, data_root, rel_folder, image_name, side=None):\n",
    "        \"\"\"Combines the relative path with the data root to get the complete path of image\n",
    "        \"\"\"\n",
    "        #print(\"erg :\",data_root, rel_folder, image_name)\n",
    "        #frame_name = \"{:06d}{}\".format(int(image_name), self.image_ext)\n",
    "        #print(\"framename: \", frame_name)\n",
    "        #print(image_name)\n",
    "        path = (os.path.join(data_root, rel_folder,\n",
    "                            \"image_0{}\".format(self.side_map[side]), \"images\", image_name)+ self.image_ext)\n",
    "        #print(\"image: \" ,path)\n",
    "        #print(\"\\n\")\n",
    "\n",
    "        return path\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        #image = self.resize(image)\n",
    "        # if self.image_aug and random.random() > self.aug_prob: image = self.image_aug(image)\n",
    "        if self.image_aug: image = self.image_aug(image=np.asarray(image))[\"image\"]  # alb needs np input\n",
    "        return func.to_tensor(np.array(image))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns the image with pre-proc transforms + aug applied to it\"\"\"\n",
    "        args = self.get_split_filename(self.filenames[index])\n",
    "        #print(\"*args: \", args)\n",
    "        #print(self.make_image_path(self.data_root_folder, *args))\n",
    "        image = self.image_loader(self.make_image_path(self.data_root_folder, *args))\n",
    "        image = self.preprocess(image)\n",
    "        return image, self.filenames[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631699c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset Monovision (with BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c2ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EndoMaskDataset(EndoDataset):\n",
    "    \"\"\" Loads an image and its corresponding mask\n",
    "    Some aug is performed common to both image and mask\n",
    "    Some aug like color aug is performed only on the image\n",
    "    :returns Pre-proc+aug image, mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_transform=None,\n",
    "                 image_mask_aug=None,\n",
    "                 mask_path_suffix=\"\",\n",
    "                 **kwargs):\n",
    "        super(EndoMaskDataset, self).__init__(**kwargs)\n",
    "        self.mask_path_suffix = mask_path_suffix\n",
    "        self.image_mask_aug = image_mask_aug\n",
    "        self.mask_loader = mask_np_loader if self.image_ext == '.npy' else mask_loader\n",
    "        self.mask_transform = transforms.ToTensor() if mask_transform is None else transforms.Compose(mask_transform)\n",
    "\n",
    "    def make_mask_path(self, data_root, rel_folder, image_name, side=None):\n",
    "        \"\"\"Combines the relative path with the data root to get the complete path of mask\n",
    "        \"\"\"\n",
    "        #frame_name = \"{:06d}{}\".format(int(image_name), self.image_ext)\n",
    "        #return os.path.join(data_root, rel_folder,\n",
    "        #                    \"image_0{}\".format(self.side_map[side]), \"annotations\", \"instances_default.json\")#+self.mask_path_suffix\n",
    "        path = os.path.join(data_root, rel_folder,\n",
    "                            \"image_0{}\".format(self.side_map[side]), \"annotations\", \"instances_default.json\")\n",
    "        return path\n",
    "\n",
    "    def preprocess_image_mask(self, image, mask):\n",
    "        #image = self.resize(image)\n",
    "        if self.image_aug: image = self.image_aug(image=np.asarray(image))[\"image\"]\n",
    "        if self.image_mask_aug:\n",
    "            bboxes = [ann['bboxes'] for ann in mask]\n",
    "            labels = [ann['labels'] for ann in mask]\n",
    "\n",
    "            augmented = self.image_mask_aug(image=np.asarray(image), bboxes=bboxes, class_labels=labels)\n",
    "            # alb needs np input\n",
    "            image = augmented[\"image\"]\n",
    "            boxes = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in augmented['bboxes']]\n",
    "            labels = augmented['class_labels']\n",
    "            \n",
    "            # Nicht existente Box mit passender Größe\n",
    "            if len(boxes) == 0:\n",
    "                boxes = np.zeros((0,4))\n",
    "                labels = np.zeros((0), dtype=np.int64)\n",
    "            \n",
    "            mask = {'boxes': torch.tensor(boxes), 'labels': torch.tensor(labels)}\n",
    "            \n",
    "            #mask = [{'bboxes': box, 'labels': label} for box, label in zip(augmented['bboxes'], augmented['class_labels'])]\n",
    "        image = func.to_tensor(np.array(image))\n",
    "\n",
    "        #mask = [{'boxes': torch.tensor([m['bboxes']]), 'labels': torch.tensor(m['labels'])} for m in mask]\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #find json for one video and get boundingbox and category\n",
    "        #targets = [{'boxes': bbox, 'labels': label}]\n",
    "        #return img, targets\n",
    "        args = self.get_split_filename(self.filenames[index])\n",
    "        #print(\"*args: \", args)\n",
    "        image_path = self.make_image_path(self.data_root_folder, *args)\n",
    "        #print(\"image_path\", image_path)\n",
    "        image = self.image_loader(image_path)\n",
    "        mask_path = self.make_mask_path(self.data_root_folder, *args)\n",
    "        #print(\"mask_path\", mask_path)\n",
    "        mask = self.mask_loader(mask_path, args[1])\n",
    "        image, mask = self.preprocess_image_mask(image=image, mask=mask)\n",
    "        return image, mask, self.filenames[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b869214",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset Stereovision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ffb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StereoEndoDataset(EndoDataset):\n",
    "    def get_split_filename(self, filename):\n",
    "        \"\"\" Splits a filename string comprising of \"surgery_number <space> image_name <space> side\"\n",
    "        :param filename A string comprising of \"relative_path <space> image_name\"\n",
    "        :return split_filename- \"surgery_number, image_name\"\n",
    "        \"\"\"\n",
    "        split_filename = filename.split()\n",
    "        if len(split_filename) == 2: return split_filename[0], split_filename[1]\n",
    "        else: return split_filename\n",
    "\n",
    "    def preprocess(self, image_left, image_right):\n",
    "        image_left = self.resize(image_left)\n",
    "        image_right = self.resize(image_right)\n",
    "        # if self.image_aug and random.random() > self.aug_prob: image = self.image_aug(image)\n",
    "        if self.image_aug: image_left = self.image_aug(image=np.asarray(image_left))[\"image\"]  # alb needs np input\n",
    "        if self.image_aug: image_right = self.image_aug(image=np.asarray(image_right))[\"image\"]  # alb needs np input\n",
    "        image = np.concatenate((np.array(image_left), np.array(image_right)), axis=2)\n",
    "        return func.to_tensor(image)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns the image with pre-proc transforms + aug applied to it\"\"\" \n",
    "        args = self.get_split_filename(self.filenames[index])\n",
    "        image_left = self.image_loader(self.make_image_path(self.data_root_folder, *[\"l\" if idx == 3 else arg for idx, arg in enumerate(args)]))\n",
    "        image_right = self.image_loader(self.make_image_path(self.data_root_folder, *[\"r\" if idx == 3 else arg for idx, arg in enumerate(args)]))\n",
    "        image = self.preprocess(image_left, image_right)\n",
    "        return image, self.filenames[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef6220",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset Stereovision (with BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8246665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StereoEndoMaskDataset(EndoMaskDataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(StereoEndoMaskDataset, self).__init__(**kwargs)\n",
    "        self.other_cam = {\"l\": \"r\", \"r\":\"l\"}\n",
    "        \n",
    "    def get_filenumber(self, filename):\n",
    "        filenumber = filename.split('_')[2]\n",
    "        return filenumber\n",
    "     \n",
    "    def make_filename(self, surgery, filenumber, side):\n",
    "        return f\"{surgery}_0{self.side_map[str(side)]}_{filenumber}\"\n",
    "    \n",
    "    def make_image_path(self, data_root, rel_folder, image_number, side=None):\n",
    "        \"\"\"Combines the relative path with the data root to get the complete path of image\n",
    "        \"\"\"\n",
    "        side = self.side_map[side]\n",
    "        path = (os.path.join(data_root, rel_folder,\n",
    "                            f\"image_0{side}\", \"images\", self.make_filename(rel_folder, image_number, side) + self.image_ext))\n",
    "        return path\n",
    "    \n",
    "    def preprocess_image_mask(self, image_A, image_B, mask_A, mask_B):\n",
    "        if self.image_aug: image_A = self.image_aug(image=np.asarray(image_A))[\"image\"]\n",
    "        if self.image_aug: image_B = self.image_aug(image=np.asarray(image_B))[\"image\"]\n",
    "        if self.image_mask_aug:\n",
    "            # Set a seed before augmenting image A. The same seed has to be set before augmenting image B to get the same augmentation parameters.\n",
    "            seed = random.random()\n",
    "            random.seed(seed)\n",
    "            bboxes_A = [ann['bboxes'] for ann in mask_A]\n",
    "            labels_A = [ann['labels'] for ann in mask_A]\n",
    "            augmented_A = self.image_mask_aug(image=np.asarray(image_A), bboxes=bboxes_A, class_labels=labels_A)\n",
    "            \n",
    "            image_A = augmented_A[\"image\"]\n",
    "            boxes_A = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in augmented_A['bboxes']]\n",
    "            labels_A = augmented_A['class_labels']\n",
    "            \n",
    "            if len(boxes_A) == 0:\n",
    "                boxes_A = np.zeros((0,4))\n",
    "                labels_A = np.zeros((0), dtype=np.int64)\n",
    "            mask_A = {'boxes': boxes_A, 'labels': labels_A}\n",
    "            \n",
    "            random.seed(seed)\n",
    "            bboxes_B = [ann['bboxes'] for ann in mask_B]\n",
    "            labels_B = [ann['labels'] for ann in mask_B]\n",
    "            augmented_B = self.image_mask_aug(image=np.asarray(image_B), bboxes=bboxes_B, class_labels=labels_B)\n",
    "            \n",
    "            image_B = augmented_B[\"image\"]\n",
    "            boxes_B = [[box[0], box[1], box[0] + box[2], box[1] + box[3]] for box in augmented_B['bboxes']]\n",
    "            mask_B = {'boxes': boxes_B, 'labels': augmented_B['class_labels']}\n",
    "            \n",
    "        image = func.to_tensor(np.concatenate((np.array(image_A), np.array(image_B)), axis=2))\n",
    "        mask = {'boxes': torch.tensor(mask_A['boxes']), 'labels': torch.tensor(mask_A['labels'])}\n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        args = self.get_split_filename(self.filenames[index])\n",
    "        \n",
    "        surgery = args[0]\n",
    "        filenumber = self.get_filenumber(args[1])\n",
    "        side = args[2]\n",
    "        \n",
    "        image_A = self.image_loader(self.make_image_path(self.data_root_folder, *[surgery, filenumber, side]))\n",
    "        mask_A = self.mask_loader(self.make_mask_path(self.data_root_folder, *[surgery, filenumber, side]),\n",
    "                                    self.make_filename(surgery, filenumber, side))\n",
    "        image_B = self.image_loader(self.make_image_path(self.data_root_folder, *[surgery, filenumber, self.other_cam[side]]))\n",
    "        mask_B = self.mask_loader(self.make_mask_path(self.data_root_folder, *[surgery, filenumber, self.other_cam[side]]),\n",
    "                                     self.make_filename(surgery, filenumber, self.other_cam[side]))\n",
    "        image, mask = self.preprocess_image_mask(image_A,image_B, mask_A, mask_B)\n",
    "        return image, mask, self.filenames[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a121751",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug these parameters as part of your code\n",
    "HEIGHT =  256\n",
    "# what height you want the input image to the model\n",
    "WIDTH = 256 \n",
    "# what width you want the input image to the model\n",
    "AUG_PROB = 0.5 \n",
    "# probability with which some random augmentations are applied to the model\n",
    "BATCH_SIZE = 8\n",
    "# or whatever fits in memory\n",
    "NUM_WORKERS = 8\n",
    "# or 16 depending on what kind of multi-processing you want, if doubtful just use the defaults and \n",
    "#leave this to pytorch\n",
    "NUM_CLASSES = 6\n",
    "# number of classes (plus one for the background class)\n",
    "NUM_EPOCHS = 100\n",
    "# number of epochs to train the models\n",
    "NUM_CHANNELS = 6 \n",
    "# number of input channels for the ResNet50 backbone (Only needed for stereo model)\n",
    "\n",
    "IMAGE_EXT = \".png\"\n",
    "# change this only if you are trying with another image format like .npy that is faster to load\n",
    "DATAROOT = \"/mnt/sds-hd/sd22a004/guest/data_preprocessed/data_coco/\"\n",
    "# path to the images, example: /mnt/sds-hd/sd22a004/guest/dataset/instrument_detection_dataset_raw/\n",
    "BACKUP_PATH = \"/mnt/sds-hd/sd22a004/guest/object_detection/model_backups\"\n",
    "# base path to the location where model weights can be stored\n",
    "PREDICTION_BASE_PATH = '/mnt/sds-hd/sd22a004/guest/object_detection/predictions/'\n",
    "# base path to the location where predictions are stored\n",
    "SPLIT_BASE_PATH = \"/mnt/sds-hd/sd22a004/guest/data_preprocessed/splits/data_splitted/test_cv_split_lr/\"\n",
    "# base path to the location where split definitions are stored\n",
    "\n",
    "# Depending on the availability of gpu this can be changed accordingly\n",
    "if torch.cuda.device_count() > 0:\n",
    "    DEVICE = torch.device('cuda:0') #f'cuda:{torch.cuda.device_count() - 1}')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9434f21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining Augmentation\n",
    "\n",
    "Training augmentation:  \n",
    "Images have to be the same size when beeing fed into the model, so a `Resize` is necessary.  \n",
    "The provided dataset is not consistent in the lighting and sometimes the 3D-printed mount has a different color. Therefore we chose `ColorJitter`, `RandomBrightnessContrast` and `RGBShift` to adjust for these issues.  \n",
    "Some typical transformations (Flips, Rotation etc.) are used to account for changes in the camera angles an positions.  \n",
    "\n",
    "Validation augmentation:  \n",
    "We do not want images to be changed for the validation. Resizing is still necessary, so the model can handle the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = alb.Compose([alb.Resize(height=HEIGHT, width=WIDTH),\n",
    "                   alb.ColorJitter(brightness=0.2,\n",
    "                                  contrast=(0.3, 1.5),\n",
    "                                  saturation=(0.5, 2),\n",
    "                                  hue=0.1,\n",
    "                                  p=AUG_PROB),\n",
    "                   alb.HorizontalFlip(p=0.5),\n",
    "                   alb.VerticalFlip(p=0.5),\n",
    "                   alb.ShiftScaleRotate(p=0.5),\n",
    "                   alb.RandomBrightnessContrast(p=0.3, brightness_limit=0.1),\n",
    "                   alb.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3)\n",
    "                  ],\n",
    "                  bbox_params = alb.BboxParams(format = 'coco', label_fields = [\"class_labels\"])\n",
    "                )\n",
    "\n",
    "val_aug = alb.Compose([alb.Resize(height=HEIGHT, width=WIDTH),\n",
    "                      ],\n",
    "                     bbox_params = alb.BboxParams(format = 'coco', label_fields = [\"class_labels\"])\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834c355",
   "metadata": {},
   "source": [
    "## Functions for training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec886f61",
   "metadata": {},
   "source": [
    "### Creating Dataset/Dataloader\n",
    "These are wrapper functions to ease the defining of different datasets and dataloaders later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd96322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(DataSet, filenames, aug):\n",
    "    dataset = DataSet(data_root_folder=DATAROOT,\n",
    "                      filenames=filenames,\n",
    "                      height=HEIGHT,\n",
    "                      width=WIDTH,\n",
    "                      image_ext=IMAGE_EXT,\n",
    "                      image_mask_aug=aug\n",
    "                     )\n",
    "    return dataset\n",
    "\n",
    "def get_dataloader(dataset, shuffle=True):\n",
    "    return DataLoader(dataset,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      shuffle=shuffle,\n",
    "                      drop_last=False,\n",
    "                      collate_fn=collate,\n",
    "                      num_workers=NUM_WORKERS,\n",
    "                      worker_init_fn=seed_worker\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07daa5dc",
   "metadata": {},
   "source": [
    "### Creating training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, num_epochs):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.1, patience = 1, threshold = 1*np.exp(-4))\n",
    "\n",
    "    len_dataloader = len(dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        for train_i, (train_imgs, train_annotations, filenames) in enumerate(dataloader, start=1):\n",
    "            train_imgs = list(train_img.to(DEVICE) for train_img in train_imgs)\n",
    "            train_annotations = [{train_k: train_v.to(DEVICE) for train_k, train_v in train_t.items()} for train_t in train_annotations] \n",
    "\n",
    "            train_loss_dict = model(train_imgs, train_annotations)\n",
    "            train_losses = sum(train_loss for train_loss in train_loss_dict.values())\n",
    "            optimizer.zero_grad()\n",
    "            train_losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'\\rIteration: {train_i}/{len_dataloader}, Loss: {train_losses}', end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "        #scheduler Reduce LR on Plateau\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(train_losses)\n",
    "\n",
    "        # Save the model state and the number of the last trained epoch\n",
    "        torch.save(model.state_dict(), os.path.join(BACKUP_PATH, f'pretrained_{MODEL_DEF}_{FOLD}.pth'))\n",
    "        with open(os.path.join(BACKUP_PATH, f'_trained_epochs.txt'), 'w') as file:\n",
    "            file.write(f'Trained model: {MODEL_DEF}_{FOLD}\\nFinished epochs: {epoch + 1}')\n",
    "\n",
    "        # Display loss statistics\n",
    "        print(\"Epoch {}/{}, Train-Loss: {:.3f}, LR: {:.10f}\".format(epoch+1, NUM_EPOCHS, train_losses, lr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9bb66",
   "metadata": {},
   "source": [
    "### Creating validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    len_val_dataloader = len(dataloader)\n",
    "    model.eval()\n",
    "\n",
    "    val_filenames = []\n",
    "    val_gt = []\n",
    "    val_predictions = []\n",
    "\n",
    "    for val_i, (val_imgs, val_annotations, filenames) in enumerate(val_dataloader, start=1):\n",
    "        with torch.no_grad():\n",
    "            val_imgs = list(val_img.to(DEVICE) for val_img in val_imgs)\n",
    "            val_annotations = [{val_k: val_v.to(DEVICE) for val_k, val_v in val_t.items()} for val_t in val_annotations] \n",
    "            val_pred = model(val_imgs, val_annotations)\n",
    "\n",
    "            # Non-Maximum-Supression\n",
    "            val_pred_reduced = []\n",
    "            for pred in val_pred:\n",
    "                nms_preds = {'boxes': torch.Tensor([]).to(DEVICE), 'labels': torch.Tensor([]).to(DEVICE), 'scores': torch.Tensor([]).to(DEVICE)}\n",
    "                for class_predictions in split_classes(pred):\n",
    "                    class_predictions = apply_nms(class_predictions, 0.1) # TODO: kleinerer Threshold\n",
    "\n",
    "                    nms_preds['boxes'] = torch.cat((nms_preds['boxes'], class_predictions['boxes']), 0)\n",
    "                    nms_preds['labels'] = torch.cat((nms_preds['labels'], class_predictions['labels']), 0)\n",
    "                    nms_preds['scores'] = torch.cat((nms_preds['scores'], class_predictions['scores']), 0)\n",
    "                \n",
    "                val_pred_reduced.append(nms_preds)\n",
    "\n",
    "            val_filenames.extend(filenames)\n",
    "            val_gt.extend(val_annotations)\n",
    "            val_predictions.extend(val_pred_reduced)\n",
    "\n",
    "\n",
    "            print(f'\\rIteration: {val_i}/{len_val_dataloader}', end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    path = os.path.join(PREDICTION_BASE_PATH, f\"predictions_{MODEL_DEF}_{FOLD}.json\")\n",
    "    save_predictions_to_json(val_filenames, val_gt, val_predictions, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d08063",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Code for Stereovision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55161cbe",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model needs the means and standard deviations for a normalization step. Usually they are predifined when using only 3 channels.\n",
    "# For the stereo model they need to be specified for 6 channels. This function uses one batch to get an approximation of the values, which should be enough.\n",
    "def get_means_and_std(dataloader):\n",
    "    imgs, _, _ = next(iter(dataloader))\n",
    "    means = np.mean([[c.mean().item() for c in img] for img in imgs], axis=0)\n",
    "    stds = np.std([[c.std().item() for c in img] for img in imgs], axis=0)\n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stereo_model(n_classes, n_channels=6, means=[0.485, 0.456, 0.406, 0.485, 0.456, 0.406], stds=[0.229, 0.224, 0.225, 0.229, 0.224, 0.225]):\n",
    "    # image_means and image_sts need arrays in the length n_channels. For convenience this is calculated beforehand (Could also use standard values and expand them)\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True, image_mean=means, image_std=stds)\n",
    "\n",
    "    # Since the new input layer will have randomly initialized parameters, we reuse the pretrained ones and duplicate them onto the additional 3 input channels\n",
    "    pretrained_params_input_layer = next(model.parameters())\n",
    "    params_combined = torch.cat([pretrained_params_input_layer, pretrained_params_input_layer], dim=1)\n",
    "    \n",
    "    # number of channels has to be adjusted for the input layer\n",
    "    model.backbone.body.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    # Replace the randomly initialized weights\n",
    "    model.backbone.body.conv1.weight = nn.Parameter(params_combined, requires_grad=False)\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, n_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ccc98c",
   "metadata": {},
   "source": [
    "## Iterate Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55539002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Iteration: 267/267, Loss: 0.220802431350540376\n",
      "Epoch 1/100, Train-Loss: 0.221, LR: 0.0050000000\n",
      "Epoch: 2\n",
      "Iteration: 267/267, Loss: 0.014507347717881203\n",
      "Epoch 2/100, Train-Loss: 0.015, LR: 0.0050000000\n",
      "Epoch: 3\n",
      "Iteration: 267/267, Loss: 0.090522070312682586\n",
      "Epoch 3/100, Train-Loss: 0.091, LR: 0.0050000000\n",
      "Epoch: 4\n",
      "Iteration: 267/267, Loss: 0.192345406811887755\n",
      "Epoch 4/100, Train-Loss: 0.192, LR: 0.0050000000\n",
      "Epoch: 5\n",
      "Iteration: 267/267, Loss: 0.155872911650890434\n",
      "Epoch 5/100, Train-Loss: 0.156, LR: 0.0005000000\n",
      "Epoch: 6\n",
      "Iteration: 267/267, Loss: 0.144506101995987936\n",
      "Epoch 6/100, Train-Loss: 0.145, LR: 0.0005000000\n",
      "Epoch: 7\n",
      "Iteration: 267/267, Loss: 0.078974601845248614\n",
      "Epoch 7/100, Train-Loss: 0.079, LR: 0.0000500000\n",
      "Epoch: 8\n",
      "Iteration: 267/267, Loss: 0.182767569886499654\n",
      "Epoch 8/100, Train-Loss: 0.183, LR: 0.0000500000\n",
      "Epoch: 9\n",
      "Iteration: 267/267, Loss: 0.069765631104917072\n",
      "Epoch 9/100, Train-Loss: 0.070, LR: 0.0000050000\n",
      "Epoch: 10\n",
      "Iteration: 267/267, Loss: 0.124208062921839214\n",
      "Epoch 10/100, Train-Loss: 0.124, LR: 0.0000050000\n",
      "Epoch: 11\n",
      "Iteration: 267/267, Loss: 0.125065910049786966\n",
      "Epoch 11/100, Train-Loss: 0.125, LR: 0.0000005000\n",
      "Epoch: 12\n",
      "Iteration: 267/267, Loss: 0.038353054680288724\n",
      "Epoch 12/100, Train-Loss: 0.038, LR: 0.0000005000\n",
      "Epoch: 13\n",
      "Iteration: 267/267, Loss: 0.055340320836251765\n",
      "Epoch 13/100, Train-Loss: 0.055, LR: 0.0000000500\n",
      "Epoch: 14\n",
      "Iteration: 267/267, Loss: 0.121154420184252824\n",
      "Epoch 14/100, Train-Loss: 0.121, LR: 0.0000000500\n",
      "Epoch: 15\n",
      "Iteration: 267/267, Loss: 0.0076209744438529015\n",
      "Epoch 15/100, Train-Loss: 0.008, LR: 0.0000000050\n",
      "Epoch: 16\n",
      "Iteration: 267/267, Loss: 0.205631970256650235\n",
      "Epoch 16/100, Train-Loss: 0.206, LR: 0.0000000050\n",
      "Epoch: 17\n",
      "Iteration: 267/267, Loss: 0.013858582824468613\n",
      "Epoch 17/100, Train-Loss: 0.014, LR: 0.0000000050\n",
      "Epoch: 18\n",
      "Iteration: 267/267, Loss: 0.175001710762966475\n",
      "Epoch 18/100, Train-Loss: 0.175, LR: 0.0000000050\n",
      "Epoch: 19\n",
      "Iteration: 267/267, Loss: 0.211167566185313526\n",
      "Epoch 19/100, Train-Loss: 0.211, LR: 0.0000000050\n",
      "Epoch: 20\n",
      "Iteration: 267/267, Loss: 0.185588530378628574\n",
      "Epoch 20/100, Train-Loss: 0.186, LR: 0.0000000050\n",
      "Epoch: 21\n",
      "Iteration: 267/267, Loss: 0.008294755592942238\n",
      "Epoch 21/100, Train-Loss: 0.008, LR: 0.0000000050\n",
      "Epoch: 22\n",
      "Iteration: 267/267, Loss: 0.135837162683277287\n",
      "Epoch 22/100, Train-Loss: 0.136, LR: 0.0000000050\n",
      "Epoch: 23\n",
      "Iteration: 267/267, Loss: 0.053055584394220576\n",
      "Epoch 23/100, Train-Loss: 0.053, LR: 0.0000000050\n",
      "Epoch: 24\n",
      "Iteration: 267/267, Loss: 0.0949117534946811955\n",
      "Epoch 24/100, Train-Loss: 0.095, LR: 0.0000000050\n",
      "Epoch: 25\n",
      "Iteration: 267/267, Loss: 0.066551671215669741\n",
      "Epoch 25/100, Train-Loss: 0.067, LR: 0.0000000050\n",
      "Epoch: 26\n",
      "Iteration: 267/267, Loss: 0.230926476112502441\n",
      "Epoch 26/100, Train-Loss: 0.231, LR: 0.0000000050\n",
      "Epoch: 27\n",
      "Iteration: 267/267, Loss: 0.011617887765169144\n",
      "Epoch 27/100, Train-Loss: 0.012, LR: 0.0000000050\n",
      "Epoch: 28\n",
      "Iteration: 267/267, Loss: 0.070325947445182115\n",
      "Epoch 28/100, Train-Loss: 0.070, LR: 0.0000000050\n",
      "Epoch: 29\n",
      "Iteration: 267/267, Loss: 0.103055548679378415\n",
      "Epoch 29/100, Train-Loss: 0.103, LR: 0.0000000050\n",
      "Epoch: 30\n",
      "Iteration: 267/267, Loss: 0.118538890055753374\n",
      "Epoch 30/100, Train-Loss: 0.119, LR: 0.0000000050\n",
      "Epoch: 31\n",
      "Iteration: 267/267, Loss: 0.169732943909127367\n",
      "Epoch 31/100, Train-Loss: 0.170, LR: 0.0000000050\n",
      "Epoch: 32\n",
      "Iteration: 267/267, Loss: 0.094873124723545823\n",
      "Epoch 32/100, Train-Loss: 0.095, LR: 0.0000000050\n",
      "Epoch: 33\n",
      "Iteration: 267/267, Loss: 0.133920302694790976\n",
      "Epoch 33/100, Train-Loss: 0.134, LR: 0.0000000050\n",
      "Epoch: 34\n",
      "Iteration: 59/267, Loss: 0.1353659862866683575"
     ]
    }
   ],
   "source": [
    "# Each model should have its own definition if you wish to store them for later evaluation. Each fold gets a corresponding suffix so one model is saved for each fold.\n",
    "MODEL_DEF = 'stereo_100Epoch'\n",
    "\n",
    "for n_fold in range(1, 5):\n",
    "    FOLD = f'fold_{n_fold}'\n",
    "    split_file_path = os.path.join(SPLIT_BASE_PATH, FOLD, \"{}_files.txt\")\n",
    "\n",
    "    train_filenames = read_lines_from_text_file(split_file_path.format(\"train\"))\n",
    "    train_dataloader = get_dataloader(get_dataset(StereoEndoMaskDataset, train_filenames, aug))\n",
    "\n",
    "    val_filenames = read_lines_from_text_file(split_file_path.format(\"val\"))\n",
    "    val_dataloader = get_dataloader(get_dataset(StereoEndoMaskDataset, val_filenames, val_aug), shuffle=False)\n",
    "    \n",
    "    means, stds = get_means_and_std(train_dataloader)\n",
    "    model = get_stereo_model(NUM_CLASSES, NUM_CHANNELS, means, stds)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    train(model, train_dataloader, NUM_EPOCHS)\n",
    "    evaluate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8c9be",
   "metadata": {},
   "source": [
    "# Code for Monovision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1645b",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e257c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_object_annotation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cef16",
   "metadata": {},
   "source": [
    "## Iterate Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c585c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each model should have its own definition if you wish to store them for later evaluation. Each fold gets a corresponding suffix so one model is saved for each fold.\n",
    "MODEL_DEF = 'mono_100Epochs'\n",
    "\n",
    "for n_fold in range(1, 5):\n",
    "    FOLD = f'fold_{n_fold}'\n",
    "    split_file_path = os.path.join(SPLIT_BASE_PATH, FOLD, \"{}_files.txt\")\n",
    "\n",
    "    train_filenames = read_lines_from_text_file(split_file_path.format(\"train\"))\n",
    "    train_dataloader = get_dataloader(get_dataset(EndoMaskDataset, train_filenames, aug))\n",
    "\n",
    "    val_filenames = read_lines_from_text_file(split_file_path.format(\"val\"))\n",
    "    val_dataloader = get_dataloader(get_dataset(EndoMaskDataset, val_filenames, val_aug))\n",
    "    \n",
    "    model = get_model_object_annotation(NUM_CLASSES)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    train(model, train_dataloader, NUM_EPOCHS)\n",
    "    evaluate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8b25a",
   "metadata": {},
   "source": [
    "## Plot augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70dd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, train_annotations, filenames = next(iter(train_dataloader))\n",
    "\n",
    "fig = plt.figure(figsize=(10,25))\n",
    "for i in range(0, BATCH_SIZE): \n",
    "    ax = plt.subplot(8, 2, i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(filenames[i])\n",
    "    plt.imshow(np.transpose(train_imgs[i], (1, 2, 0)))\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8849aa",
   "metadata": {},
   "source": [
    "# Code for predicting on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a685856",
   "metadata": {},
   "source": [
    "### Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT_PATH = \"/mnt/sds-hd/sd22a004/guest/data_preprocessed/splits/data_splitted_testset/test_cv_split_lr/test_file.txt\"\n",
    "TEST_DATAROOT = \"/mnt/sds-hd/sd22a004/guest/data_preprocessed/data_testset/\"\n",
    "MODEL_PATH = \"/mnt/sds-hd/sd22a004/guest/trained_models/pretrained_mono_100epochs_fold_1.pth\"\n",
    "MODEL_DEF = \"mono_100epochs\"\n",
    "FOLD = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d8446",
   "metadata": {},
   "source": [
    "### Creating prediction function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e76601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    len_test_dataloader = len(dataloader)\n",
    "    model.eval()\n",
    "\n",
    "    test_filenames = []\n",
    "    test_predictions = []\n",
    "\n",
    "    for test_i, (test_imgs, filenames) in enumerate(dataloader, start=1):\n",
    "        with torch.no_grad():\n",
    "            test_imgs = list(test_img.to(DEVICE) for test_img in test_imgs)\n",
    "            test_pred = model(test_imgs)\n",
    "\n",
    "            # Non-Maximum-Supression\n",
    "            test_pred_reduced = []\n",
    "            for pred in test_pred:\n",
    "                nms_preds = {'boxes': torch.Tensor([]).to(DEVICE), 'labels': torch.Tensor([]).to(DEVICE), 'scores': torch.Tensor([]).to(DEVICE)}\n",
    "                for class_predictions in split_classes(pred):\n",
    "                    class_predictions = apply_nms(class_predictions, 0.1)\n",
    "\n",
    "                    nms_preds['boxes'] = torch.cat((nms_preds['boxes'], class_predictions['boxes']), 0)\n",
    "                    nms_preds['labels'] = torch.cat((nms_preds['labels'], class_predictions['labels']), 0)\n",
    "                    nms_preds['scores'] = torch.cat((nms_preds['scores'], class_predictions['scores']), 0)\n",
    "                \n",
    "                test_pred_reduced.append(nms_preds)\n",
    "\n",
    "            test_filenames.extend(filenames)\n",
    "            test_predictions.extend(test_pred_reduced)\n",
    "\n",
    "\n",
    "            print(f'\\rIteration: {test_i}/{len_test_dataloader}', end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    path = os.path.join(PREDICTION_BASE_PATH, f\"predictions{MODEL_DEF}_{FOLD}.json\")\n",
    "    save_predictions_to_json(test_filenames, [], test_predictions, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960049c",
   "metadata": {},
   "source": [
    "### Building/Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_object_annotation(NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec029f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = read_lines_from_text_file(TEST_SPLIT_PATH)\n",
    "\n",
    "test_dataset = EndoDataset(data_root_folder=TEST_DATAROOT,\n",
    "                      filenames=test_filenames,\n",
    "                      height=HEIGHT,\n",
    "                      width=WIDTH,\n",
    "                      image_ext=IMAGE_EXT,\n",
    "                      image_aug=alb.Compose([alb.Resize(height=HEIGHT, width=WIDTH)])\n",
    "                     )\n",
    "\n",
    "test_dataloader = get_dataloader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048b01b",
   "metadata": {},
   "source": [
    "# Additional Content for future implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bc03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': tensor(0.1248),\n",
      " 'map_50': tensor(0.3331),\n",
      " 'map_75': tensor(0.0629),\n",
      " 'map_large': tensor(0.1676),\n",
      " 'map_medium': tensor(0.0795),\n",
      " 'map_per_class': tensor([ 0.2363, -1.0000,  0.0134, -1.0000]),\n",
      " 'map_small': tensor(0.0032),\n",
      " 'mar_1': tensor(0.2499),\n",
      " 'mar_10': tensor(0.3000),\n",
      " 'mar_100': tensor(0.3000),\n",
      " 'mar_100_per_class': tensor([ 0.4028, -1.0000,  0.1972, -1.0000]),\n",
      " 'mar_large': tensor(0.3633),\n",
      " 'mar_medium': tensor(0.2367),\n",
      " 'mar_small': tensor(0.0500)}\n"
     ]
    }
   ],
   "source": [
    "# Can be used in the validation run\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "metric = MeanAveragePrecision(class_metrics=True)\n",
    "metric.to(DEVICE)\n",
    "metric.update(val_predictions, val_gt)\n",
    "\n",
    "pprint(metric.compute())"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Bastian Westerheide"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b88b42fbddda55578e7dd4e35ba7e46c8dd4d9b2415f9d2d8f7c1d51903a65f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
